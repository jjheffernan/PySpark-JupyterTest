{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca641f63",
   "metadata": {},
   "source": [
    "# Apache Spark's Structured APIs\n",
    "\n",
    "## Spark: What's Underneath an RDD?\n",
    "\n",
    "The RDD is the most basic abstraction in Spark. There are three vital characteristics to an RDD.\n",
    "* Dependencies\n",
    "* Partitions (with some locality information)\n",
    "* Compute function: Partiotion=> Iterator[T]\n",
    "\n",
    "All three are intergral to the simple RDD programming API model upon which all higher-level functionality is constructed.\n",
    "1. A list of *dependencies* that instincts Spark how an RDD is constructed with its inputs is required. When Reproducing Results, Spark can remake RDDS from these dependencies and replicate the operations. It gives the RDDs **Resiliency**\n",
    "2. *Partitions* Provide Spark the ability to split the work to parallelize computation on partitions across executors. Spark sometimes (HDFS and others) will us locality information to send work to executors close to the data. This reduces data transmitted over the network\n",
    "3. RDDs also have a *compute function* that produces an `Iterator [T]` for the data that will be stored in the RDD.\n",
    "\n",
    "Simple and Elegant! There arise a couple of problems with this model though. Spark does not know *what* you are doing in the compute function. In other terms, the compute function is **opaque** to Spark. This means that joins, filters, selects, or aggreations are seen by Spark mostly as `lambda` expressions. \n",
    "\n",
    "Another problem also arises with Python RDDs; Spark sees the `Iterator [T]` data type as opaque. Spark only registers the Iterator as a generic Python object.\n",
    "\n",
    "## Structuring Spark\n",
    "\n",
    "There are a few schemes to structure spark. \n",
    "- express computations by using common patterns found in data analysis. (avg, filters, selects, aggregates, etc.)\n",
    "- Can also use a set of common operators within a DSL, in the form of APIs in a Spark compatible language. Very specific\n",
    "- Also, can use a order and structure scheme. This allows data to be arranged in a tabular format. like SQL tables or a spreadsheet. it has its own supported datatypes. \n",
    "\n",
    "### Key Merits and Benefits\n",
    "\n",
    "Structure yields a number of benefits, including better performance and space effi‐ ciency across Spark components. We will explore these benefits further when we talk about the use of the DataFrame and Dataset APIs shortly, but for now we’ll concen‐ trate on the other advantages: expressivity, simplicity, composability, and uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337d37ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scala (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for scala\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949ae0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda7ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD of tuples (name, age)\n",
    "dataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    "                         (\"TD\", 35), (\"Brooke\", 25)])\n",
    "# Use map and reduceByKey transformations with their lambda\n",
    "# expressions to aggregate and then compute average\n",
    "\n",
    "agesRDD = (dataRDD\n",
    "          .map(lambda x: (x[0], (x[1], 1)))\n",
    "          .reduceByKey(lambda x, y: (x[0]+y[0], x[1] + y[1]))\n",
    "          .map(lambda x: (x[0], x[1][0]/x[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3361b029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Denny|    31.0|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame using SparkSession \n",
    "spark = (SparkSession\n",
    "      .builder\n",
    "      .appName(\"AuthorsAges\")\n",
    "      .getOrCreate())\n",
    "\n",
    "# Create a DataFrame\n",
    "data_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    "                                 (\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n",
    "\n",
    "# Group the same names together, aggregate their ages, and compute an average \n",
    "avg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "# Show the results of the final execution\n",
    "avg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b233335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
