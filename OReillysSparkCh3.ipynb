{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca641f63",
   "metadata": {},
   "source": [
    "# Apache Spark's Structured APIs\n",
    "\n",
    "## Spark: What's Underneath an RDD?\n",
    "\n",
    "The RDD is the most basic abstraction in Spark. There are three vital characteristics to an RDD.\n",
    "* Dependencies\n",
    "* Partitions (with some locality information)\n",
    "* Compute function: Partiotion=> Iterator[T]\n",
    "\n",
    "All three are intergral to the simple RDD programming API model upon which all higher-level functionality is constructed.\n",
    "1. A list of *dependencies* that instincts Spark how an RDD is constructed with its inputs is required. When Reproducing Results, Spark can remake RDDS from these dependencies and replicate the operations. It gives the RDDs **Resiliency**\n",
    "2. *Partitions* Provide Spark the ability to split the work to parallelize computation on partitions across executors. Spark sometimes (HDFS and others) will us locality information to send work to executors close to the data. This reduces data transmitted over the network\n",
    "3. RDDs also have a *compute function* that produces an `Iterator [T]` for the data that will be stored in the RDD.\n",
    "\n",
    "Simple and Elegant! There arise a couple of problems with this model though. Spark does not know *what* you are doing in the compute function. In other terms, the compute function is **opaque** to Spark. This means that joins, filters, selects, or aggreations are seen by Spark mostly as `lambda` expressions. \n",
    "\n",
    "Another problem also arises with Python RDDs; Spark sees the `Iterator [T]` data type as opaque. Spark only registers the Iterator as a generic Python object.\n",
    "\n",
    "## Structuring Spark\n",
    "\n",
    "There are a few schemes to structure spark. \n",
    "- express computations by using common patterns found in data analysis. (avg, filters, selects, aggregates, etc.)\n",
    "- Can also use a set of common operators within a DSL, in the form of APIs in a Spark compatible language. Very specific\n",
    "- Also, can use a order and structure scheme. This allows data to be arranged in a tabular format. like SQL tables or a spreadsheet. it has its own supported datatypes. \n",
    "\n",
    "### Key Merits and Benefits\n",
    "\n",
    "Structure yields a number of benefits, including better performance and space effi‐ ciency across Spark components. We will explore these benefits further when we talk about the use of the DataFrame and Dataset APIs shortly, but for now we’ll concen‐ trate on the other advantages: expressivity, simplicity, composability, and uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337d37ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scala (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for scala\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949ae0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda7ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD of tuples (name, age)\n",
    "dataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    "                         (\"TD\", 35), (\"Brooke\", 25)])\n",
    "# Use map and reduceByKey transformations with their lambda\n",
    "# expressions to aggregate and then compute average\n",
    "\n",
    "agesRDD = (dataRDD\n",
    "          .map(lambda x: (x[0], (x[1], 1)))\n",
    "          .reduceByKey(lambda x, y: (x[0]+y[0], x[1] + y[1]))\n",
    "          .map(lambda x: (x[0], x[1][0]/x[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3361b029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Denny|    31.0|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame using SparkSession \n",
    "spark = (SparkSession\n",
    "      .builder\n",
    "      .appName(\"AuthorsAges\")\n",
    "      .getOrCreate())\n",
    "\n",
    "# Create a DataFrame\n",
    "data_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    "                                 (\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n",
    "\n",
    "# Group the same names together, aggregate their ages, and compute an average \n",
    "avg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "# Show the results of the final execution\n",
    "avg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e6cf5",
   "metadata": {},
   "source": [
    "This was accomplished via high-level DSL operators and APIs to tell Spark what to do. Spark can inspect or parse this query and understand our intention, it can optimize or arrange the operations for efficient execution. Spark knows exactly what we wish to do: group people by their names, aggregate their ages, and then compute the aver‐ age age of all people with the same name.\n",
    "\n",
    "by using only high-level, expressive DSL operators mapped to common or recurring data analysis patterns to introduce order and structure, we are limiting the scope of the developers’ ability to instruct the compiler or control how their queries should be computed.\n",
    "\n",
    "you are not confined to these structured patterns; you can switch back at any time to the unstructured low- level RDD API, although we hardly ever find a need to do so. As well as being simpler to read, the structure of Spark’s high-level APIs also introdu‐ ces uniformity across its components and languages. For example, the Scala code shown here does the same thing as the previous Python code—and the API looks nearly identical:\n",
    "\n",
    "    // In Scala\n",
    "    import org.apache.spark.sql.functions.avg import org.apache.spark.sql.SparkSession // Create a DataFrame using SparkSession val spark = SparkSession\n",
    "    .builder\n",
    "    .appName(\"AuthorsAges\")\n",
    "    .getOrCreate()\n",
    "    // Create a DataFrame of names and ages\n",
    "    val dataDF = spark.createDataFrame(Seq((\"Brooke\", 20), (\"Brooke\", 25), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35))).toDF(\"name\", \"age\")\n",
    "    // Group the same names together, aggregate their ages, and compute an average\n",
    "    val avgDF = dataDF.groupBy(\"name\").agg(avg(\"age\")) \n",
    "    // Show the results of the final execution\n",
    "    avgDF.show()\n",
    "    \n",
    "Some of these DSL operators perform relational-like operations that you’ll be familiar with if you know SQL, such as selecting, fil‐ tering, grouping, and aggregation. All of this simplicity and expressivity that we developers cherish is possible because of the Spark SQL engine upon which the high-level Structured APIs are built. It is because of this engine, which underpins all the Spark components, that we get uniform APIs. Whether you express a query against a DataFrame in Structured Stream‐ ing or MLlib, you are always transforming and operating on DataFrames as structured data\n",
    "\n",
    "## The DataFrame API\n",
    "\n",
    "Inspired by pandas DataFrames in structure, format, and a few specific operations, Spark DataFrames are like distributed in-memory tables with named columns and schemas, where each column has a specific data type: integer, string, array, map, real, date, timestamp, etc.\n",
    "\n",
    "Example Spark DataFrame Below\n",
    "\n",
    "| Id (Int) | First (String) | Last (String) | Url (String) | Published (Date) | Hits (Int) | Campaign (List[Strings]) |\n",
    "| -------- | ---------- | ----------- | --------- | -------------- | --------- | ----------- |\n",
    "| 1 | Jules | Damji | https://tinyurl.1 | 1/4/2016 | 4535 | [twitter,LinkedIn] | \n",
    "| 2 | Brooke | Wenig | https://tinyurl.2 | 5/5/2018 | 8908 | [twitter,LinkedIn] | \n",
    "| 3 | Denny | Lee | https://tinyurl.3 | 6/7/2019 | 7659 | [web, twitter, FB, LinkedIn] | \n",
    "| 4 | Tathagata | Das | https://tinyurl.4 | 5/12/2018 | 10568 | [twitter, FB] | \n",
    "| 5 | Matei | Zaharia | https://tinyurl.5 | 5/14/2014 | 40578 | [web, twitter, FB, LinkedIn] |\n",
    "| 6 | Reynold | Xin | https://tinyurl.6 | 3/2/2015 | 25568 | [twitter, LinkedIn] |\n",
    "\n",
    "When data is visualized as a structured table, it’s not only easy to digest but also easy to work with when it comes to common operations you might want to execute on rows and columns. You can add or change the names and data types of the columns, creating new DataFrames while the previ‐ ous versions are preserved. A named column in a DataFrame and its associated Spark data type can be declared in the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3036cf51",
   "metadata": {},
   "source": [
    "### Spark's Basic Data Types\n",
    "\n",
    "Matching its supported programming languages, Spark supports basic internal data types. These data types can be declared in your Spark application or defined in your schema. For example, in Scala, you can define or declare a particular column name to be of type `String`, `Byte`, `Long`, or `Map`, etc. Here, we define variable names tied to a Spark data type:\n",
    "\n",
    "    SPARK_HOME/bin/spark-shell\n",
    "    scala> import org.apache.spark.sql.types._\n",
    "    import org.apache.spark.sql.types._\n",
    "    scala> val nameTypes = StringType\n",
    "    nameTypes: org.apache.spark.sql.types.StringType.type = StringType \n",
    "    scala> val firstName = nameTypes\n",
    "    firstName: org.apache.spark.sql.types.StringType.type = StringType \n",
    "    scala> val lastName = nameTypes\n",
    "    lastName: org.apache.spark.sql.types.StringType.type = StringType\n",
    " \n",
    "Scala basic Data Types\n",
    "| Data type | Value assigned in Scala | API to instantiate | \n",
    "| --------- | ------------ | ------------ | \n",
    "| `ByteType` | `Byte` | `DataTypes.ByteType` |\n",
    "| `ShortType` | `Short` | `DataTypes.ShortType` | \n",
    "| `IntegerType` | `Int` | `DataTypes.IntegerType` | \n",
    "| `LongType` | `Long` | `DataTypes.LongType` | \n",
    "| `FloatType` | `Float` | `DataTypes.FloatType` | \n",
    "| `DoubleType` | `Double` | `DataTypes.DoubleType` | \n",
    "| `StringType` | `String` | `DataTypes.StringType` | \n",
    "| `BooleanType` | `Boolean` | `DataTypes.BooleanType` | \n",
    "| `DecimalType` | `java.math.BigDecimal` | `DecimalType` |\n",
    "\n",
    "Python Basic Data Types\n",
    "| Data type | Value assigned in Python | API to instantiate | \n",
    "| --------- | ------------ | ------------ | \n",
    "| `ByteType` | `int` | `DataTypes.ByteType` |\n",
    "| `ShortType` | `int` | `DataTypes.ShortType` | \n",
    "| `IntegerType` | `int` | `DataTypes.IntegerType` | \n",
    "| `LongType` | `int` | `DataTypes.LongType` | \n",
    "| `FloatType` | `float` | `DataTypes.FloatType` | \n",
    "| `DoubleType` | `float` | `DataTypes.DoubleType` | \n",
    "| `StringType` | `str` | `DataTypes.StringType` | \n",
    "| `BooleanType` | `bool` | `DataTypes.BooleanType` | \n",
    "| `DecimalType` | `decimal.Decimal` | `DecimalType` |\n",
    "\n",
    "### Spark's Structured and Complex Data Types\n",
    "\n",
    "For complex data analysitcs, you won't deal only with simple or basic data types. Your data will be complex, often structured or nested, and you'll need Spark to handle these complex data types. They come in many forms: maps, arrays, structs, dates, timestamps, fields, etc. \n",
    "\n",
    "Scala structured data types in Spark\n",
    "| Data type | Value assigned in Scala | API to instantiate | \n",
    "| --------- | ------------ | ------------ | \n",
    "| `BinaryType` | `Array[Byte]` | `DataTypes.BinaryType` |\n",
    "| `TimestampType` | `java.sql.Timestamp` | `DataTypes.TimestampType` | \n",
    "| `DateType` | `java.sql.Date` | `DataTypes.DateType` | \n",
    "| `ArrayType` | `scala.collection.Seq` | `DataTypes.createArrayType(ElementType)` | \n",
    "| `MapType` | `scala.collection.Map` | `DataTypes.createMapType(keyType, valueType)` | \n",
    "| `StructType` | `org.arache.spark.sql.Row` | `StructType(ArrayType[fieldTypes])` | \n",
    "| `StructField` | A value type corresponding to the type of this field | `StructField(name, dataType, [nullable])` | \n",
    "\n",
    "The equivalent structured data types in python\n",
    "| Data type | Value assigned in Python | API to instantiate | \n",
    "| --------- | ------------ | ------------ | \n",
    "| `BinaryType` | `bytearray` | `ByteType()` |\n",
    "| `TimestampType` | `datetime.datetime` | `TimestampType()` | \n",
    "| `DateType` | `datetime.date` | `DateType()` | \n",
    "| `ArrayType` | List, tuple, or array | `ArrayType(dataType, [nullable])` | \n",
    "| `MapType` | `dict` | `MapType(keyType, valueType, [nullable])` | \n",
    "| `StructType` | List or tuple | `StructType([fields])` | \n",
    "| `StructField` | A value type corresponding to the type of this | `StructField(name, dataType, [nullable])` | \n",
    "\n",
    "## Schemas and Creating DataFrames\n",
    "A *schema* in Spark defines the column names and associated data types for a DataFrame. Most often, schemas come into play when you are reading structured data from an external data source. Defining a schema up front as opposed to taking a schema-on-read approach offers three benefits:\n",
    "* You relieve Spark from the onus of inferring data types\n",
    "* You prevent Spark from creating a separate job just to read a large portion of your file to ascertain the schema, which for a large data file can be expensive and time-consuming.\n",
    "* You can detect errors early if data doesn't match the schema. \n",
    "\n",
    "### Two ways to define a schema\n",
    "\n",
    "Spark allows you to define a schema in two ways. One is to define it programmatically, and the other is to employ a Data Definition Language (DDL) string, which is much simpler and easier to read. To define a schema programmatically for a Dataframe with three named columns, `author`, `title`, and `pages`, you can use the Spark DataFrame API. \n",
    "\n",
    "    // In Scala\n",
    "    import org.apache.spark.sql.types._\n",
    "    val schema = StructType(Array(StructField(\"author\", StringType, false),\n",
    "    StructField(\"title\", StringType, false), StructField(\"pages\", IntegerType, false)))\n",
    "    \n",
    "    # In Python\n",
    "    from pyspark.sql.types import *\n",
    "    schema = StructType([StructField(\"author\", StringType(), False),\n",
    "    StructField(\"title\", StringType(), False),\n",
    "    StructField(\"pages\", IntegerType(), False)])\n",
    "    \n",
    "Defining the same schema using DDL is much simpler\n",
    "\n",
    "    // In Scala\n",
    "    val schema = \"author STRING, title STRING, pages INT\" \n",
    "    \n",
    "    # In Python\n",
    "    schema = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c9805",
   "metadata": {},
   "source": [
    "You can choose either language when defining a schema. For many examples, we will use DDLs and static schemas.\n",
    "\n",
    "    # In Python\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    # Define schema for our data using DDL\n",
    "    schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING,\n",
    "      `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "    \n",
    "    # Create our static data\n",
    "    data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\n",
    "    \"LinkedIn\"]],\n",
    "           [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "    \"LinkedIn\"]],\n",
    "           [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "    \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "           [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
    "    [\"twitter\", \"FB\"]],\n",
    "           [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "    \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "           [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
    "    [\"twitter\", \"LinkedIn\"]]\n",
    "          ]\n",
    "    \n",
    "    # Main program\n",
    "    if __name__ == \"__main__\": \n",
    "    # Create a SparkSession \n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(\"Example-3_6\")\n",
    "             .getOrCreate())\n",
    "    \n",
    "    # Create a DataFrame using the schema defined above\n",
    "    blogs_df = spark.createDataFrame(data, schema)\n",
    "    \n",
    "    # Show the DataFrame; it should reflect our table above \n",
    "    blogs_df.show()\n",
    "    # Print the schema used by Spark to process the DataFrame \n",
    "    print(blogs_df.printSchema())\n",
    "    \n",
    "If you want to use this schema elsewhere in code, simply execute `blogs_df.schema` and it will return the schema defintion:\n",
    "    StructType(List(StructField(\"Id\",IntegerType,false),\n",
    "    StructField(\"First\",StringType,false),\n",
    "    StructField(\"Last\",StringType,false),\n",
    "    StructField(\"Url\",StringType,false),\n",
    "    StructField(\"Published\",StringType,false),\n",
    "    StructField(\"Hits\",IntegerType,false),\n",
    "    StructField(\"Campaigns\",ArrayType(StringType,true),false)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b9198",
   "metadata": {},
   "source": [
    "# Columns and Expressions\n",
    "\n",
    "As mentioned, named columns in DataFrames are conceptually similar to named columns in pandas or R DataFrames or in an RDBMS table; they describe a type of field. You can list all the columns by their names, and you can perform operations on their values using relational or computational expressions. In Spark's supported languages, columns are objects with public methods (represented by the `Column` type).\n",
    "\n",
    "There also exists logical and mathematical expressions on columns. For example, `expr('columnName * 5')` or `(expr('columnName - 5') > col(anothercolumnName))`, where `columnName` is a Spark type (integer, string, etc.) `expr()` is part of the `pyspark.sql.functions` (Python) and `org.apache.spark.sql.functions` (Scala) packages. Like anoy other function in those packages, `expr()` takes arguments that Spark will parse as an expression, computing the result. \n",
    "\n",
    "Scala, Java, and Python all have public methods associated with columns. You'll note that the Spark documentation refers to both `col` and `Column`. `Column` is the name of the object, while `col()` is a standard built-in function that returns a `Column`\n",
    "\n",
    "`Column` objects in a DataFrame can't exist in isolation; each column is part of a row in a record and all the rows together constitute a DataFrame, which as we will see later in the chapter is really a `Dataset[Row]` in Scala.\n",
    "\n",
    "# Rows\n",
    "\n",
    "A row in Spark is a generic *Row object*, containing one or more columns. Each column may be of the same data type (e.g. integer or string), or they can have different types (integer, sting, map, array, etc.) Because `Row` in each of Spark's supported languages and access its field by an index starting at 0. `Row` objects can be used to create DataFrames if you need them for quick interactivity and exploration:\n",
    "\n",
    "    # In Python\n",
    "    rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "    authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "    authors_df.show()\n",
    "    \n",
    "    // In Scala\n",
    "    val rows = Seq((\"Matei Zaharia\", \"CA\"), (\"Reynold Xin\", \"CA\")) val authorsDF = rows.toDF(\"Author\", \"State\")\n",
    "    authorsDF.show()\n",
    "\n",
    "In practice, you will usually want to read DataFrames from a file as illustrated earlier. In most cases, defining a schema and using it is a quicker and more efficient way to create DataFrames. After you have created a large distibuted DataFrame, you are going to want to perform some common data operations on it. Let's examine some of the Sprk operations you can perform with high-level relational operators in the Structured APIs.\n",
    "\n",
    "# Common DataFrame Operations\n",
    "\n",
    "To perform common data operations on DataFrames, you'll first need to load a DataFrame from a data source that holds your structured data. Spark provides an interface, `DataFrameReader`, that enables you to read data into a DataFrame from myriad data sources in formats such as JSON, CSV, Parquet, Text, Avro, ORC, and more! Likewise, to write a DataFrame back to a data source in a particular format, Spark uses `DataFrameWriter`. \n",
    "\n",
    "## Using DataFrameReader and DataFrameWrite\n",
    "\n",
    "Reading and writing are simple in Spark because of these high-level abstractions and contributions from the community to connect to a wide variety of data sources, including common NoSQL stores, RDBMSs, streaming engines such as Apache Kafka and Kinesis, and more. On large files, it is more efficient to determine a schem and then have Spark infer it. If you don't want to specify the schema, Spark can infer schema from a sample at a lesser cost. For exmaple, the `samplingRatio` option:\n",
    "\n",
    "    // In Scala\n",
    "    val sampleDF = spark .read\n",
    "    .option(\"samplingRatio\", 0.001) \n",
    "    .option(\"header\", true) \n",
    "    .csv(\"\"\"/databricks-datasets/learning-spark-v2/ sf-fire/sf-fire-calls.csv\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6c0885",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Use the DataFrameReader interface to read a CSV file\u001b[39;00m\n\u001b[1;32m     35\u001b[0m sf_fire_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 36\u001b[0m fire_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msf_fire_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfire_schema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/spark-3.3.0/python/pyspark/sql/readwriter.py:535\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/opt/spark/spark-3.3.0/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/spark-3.3.0/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv"
     ]
    }
   ],
   "source": [
    " # In Python, define a schema\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "                          StructField('UnitID', StringType(), True),\n",
    "                          StructField('IncidentNumber', IntegerType(), True),\n",
    "                          StructField('CallType', StringType(), True),\n",
    "                          StructField('CallDate', StringType(), True),\n",
    "                          StructField('WatchDate', StringType(), True),\n",
    "                          StructField('CallFinalDisposition', StringType(), True),\n",
    "                          StructField('AvailableDtTm', StringType(), True),\n",
    "                          StructField('Address', StringType(), True),\n",
    "                          StructField('City', StringType(), True),\n",
    "                          StructField('Zipcode', IntegerType(), True),\n",
    "                          StructField('Battalion', StringType(), True),\n",
    "                          StructField('StationArea', StringType(), True),\n",
    "                          StructField('Box', StringType(), True),\n",
    "                          StructField('OriginalPriority', StringType(), True),\n",
    "                          StructField('Priority', StringType(), True),\n",
    "                          StructField('FinalPriority', IntegerType(), True),\n",
    "                          StructField('ALSUnit', BooleanType(), True),\n",
    "                          StructField('CallTypeGroup', StringType(), True),\n",
    "                          StructField('NumAlarms', IntegerType(), True),\n",
    "                          StructField('UnitType', StringType(), True),\n",
    "                          StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "                          StructField('FirePreventionDistrict', StringType(), True),\n",
    "                          StructField('SupervisorDistrict', StringType(), True),\n",
    "                          StructField('Neighborhood', StringType(), True),\n",
    "                          StructField('Location', StringType(), True),\n",
    "                          StructField('RowID', StringType(), True),\n",
    "                          StructField('Delay', FloatType(), True)])\n",
    "\n",
    "# Use the DataFrameReader interface to read a CSV file\n",
    "sf_fire_file = \"/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\"\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf95a6",
   "metadata": {},
   "source": [
    "In scala, this is accomplished similarly. The `spark.read.csv()` function reads the CSV file and returns a DataFrame of rows and named columns with types dictated in the schema.\n",
    "\n",
    "To write the DataFrame into an external data source in your format of choice, you can use the `DataFrameWriter` interface. Like `DataFrameReader`, it supports multiple data sources. Parquet, a popular columnar format,, is the default format; it uses snappy compression to compress the data. If the DataFrame is written as Parquet, the schema is preserved as part of the Parquet metadata. In this case, subsequent reads back into a DataFrame do not require you to manually supply a schema. \n",
    "\n",
    "### Saving a DataFrame as a Parquet file or SQL table. \n",
    "\n",
    "A common data operation is to explore and transform you data, and then persist the DataFrame in Parquet format or save it as a SQL table. Persisting a transformed DataFrame is as easy as reading it. For example, to persist the DataFrame:\n",
    "\n",
    "    // In Scala to save as a Parquet file\n",
    "    val parquetPath = ... fireDF.write.format(\"parquet\").save(parquetPath)\n",
    "    # In Python to save as a Parquet file\n",
    "    parquet_path = ...\n",
    "    fire_df.write.format(\"parquet\").save(parquet_path)\n",
    "\n",
    "Alternatively, you can save it as a table, which registers metadata with the Hive meta-store:\n",
    "\n",
    "        // In Scala to save as a table\n",
    "        val parquetTable = ... // name of the table fireDF.write.format(\"parquet\").saveAsTable(parquetTable)\n",
    "        # In Python\n",
    "        parquet_table = ... # name of the table fire_df.write.format(\"parquet\").saveAsTable(parquet_table)\n",
    "        \n",
    "note that tranformations and actions exist that can be enacted upon an existing DataFrame in a variety of methods. \n",
    "\n",
    "### Projections and Filters\n",
    "\n",
    "A *projection* in relational parlance is a way to return only the rows matching a certain relational contiditon by using filters. In Spark, projections are done with the `select()` method, while filters can be expressed using the `filter()` or `where()` method. This technique can be used to examine specific aspects of datasets. For example. \n",
    "\n",
    "    # In Python\n",
    "    few_fire_df = (fire_df\n",
    "      .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    "      .where(col(\"CallType\") != \"Medical Incident\"))\n",
    "    few_fire_df.show(5, truncate=False)\n",
    "    \n",
    "    // In Scala\n",
    "    val fewFireDF = fireDF\n",
    "        .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\") \n",
    "        .where(5\"CallType\" =!= \"Medical Incident\")\n",
    "    fewFireDF.show(5, false)\n",
    "\n",
    "What if we want to know how may distinct `CallTypes` were recorded as the causes of the fire calls? These simple and expressive queries are effective at getting the job done.\n",
    "\n",
    "    # In Python, return number of distinct types of calls using countDistinct()\n",
    "    from pyspark.sql.functions import * \n",
    "    (fire_df.select(\"CallType\")\n",
    "      .where(col(\"CallType\").isNotNull())\n",
    "      .agg(countDistinct(\"CallType\").alias(\"DistinctCallTypes\"))\n",
    "      .show())\n",
    "      \n",
    "    // In Scala\n",
    "    import org.apache.spark.sql.functions._ \n",
    "    fireDF\n",
    "      .select(\"CallType\")\n",
    "      .where(col(\"CallType\").isNotNull)\n",
    "      .agg(countDistinct('CallType) as 'DistinctCallTypes)\n",
    "      .show()\n",
    "      \n",
    "We can list the distinct call types in the data set using these queries:\n",
    "\n",
    "    # In Python, filter for only distinct non-null CallTypes from all the rows\n",
    "    (fire_df\n",
    "      .select(\"CallType\")\n",
    "      .where(col(\"CallType\").isNotNull())\n",
    "      .distinct()\n",
    "      .show(10, False))\n",
    "      \n",
    "    // In Scala\n",
    "    fireDF\n",
    "    .select(\"CallType\") .where(4\"CallType\".isNotNull()) .distinct()\n",
    "    .show(10, false)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ba997",
   "metadata": {},
   "source": [
    "### Renaming, adding, and dropping Columns\n",
    "\n",
    "Sometimes you want to rename particular columns for reasons of style or convention, and at other times for readability or brevity.  Spaces in column names can be problematic, especially when you want to write or save a DataFrame as a Parquet file (which prohibits this).\n",
    "\n",
    "By specifying the desired column names in the schema with `StructField`, as we did, we effectively changed all names in the resulting DataFrame. Alternatively, you could selectively rename columns with the `withColumnRenamed()` method. \n",
    "\n",
    "    # In Python\n",
    "    new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "    (new_fire_df\n",
    "      .select(\"ResponseDelayedinMins\")\n",
    "      .where(col(\"ResponseDelayedinMins\") > 5)\n",
    "      .show(5, False))\n",
    "      \n",
    "    // In Scala\n",
    "    val newFireDF = fireDF.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\") newFireDF\n",
    "    .select(\"ResponseDelayedinMins\") .where(4\"ResponseDelayedinMins\" > 5) .show(5, false)\n",
    "    \n",
    "This outputs a new renamed column. Because DataFrame transformations are immutable, when we rename a column using `withColumnRenamed()` we get a new DataFrame while retaining the original with the old column name. Modifying the contents of a column or its type are common operations during data exploration. In some cases the data is raw and dirty, or its types are not amenable to being supplied as arguments to relational operators. For example, in our example data, some columns will have string format dates which could be changed into a more universal datatype such as Unix timestamps or SQL dates.\n",
    "\n",
    "So how do we convert data from strings into a usable format? It's quite simple. thanks to high-level API methods. spark.sql.functions has a set of to/from date/timestamp functions such as to_timestamp() and to_date() that we can use just for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1896cb7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2351094566.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [2]\u001b[0;36m\u001b[0m\n\u001b[0;31m    fire_ts_df = (new_fire_df\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/17 18:07:56 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 301205 ms exceeds timeout 120000 ms\n",
      "22/08/17 18:07:56 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "fire_ts_df = (new_fire_df\n",
    "              .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    "              .drop(\"CallDate\")\n",
    "              .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    "              .drop(\"WatchDate\")\n",
    "              .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "              .drop(\"AvailableDtTm\"))\n",
    "\n",
    "# Select the converted columns\n",
    "(fire_ts_df\n",
    " .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    " .show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e6b3a",
   "metadata": {},
   "source": [
    "\n",
    "    // In Scala\n",
    "    val fireTsDF = newFireDF\n",
    "        .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")) .drop(\"CallDate\")\n",
    "        .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\")) .drop(\"WatchDate\")\n",
    "        .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "        .drop(\"AvailableDtTm\")\n",
    "    \n",
    "    // Select the converted columns\n",
    "    fireTsDF.select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\").show(5, false)\n",
    "    \n",
    "Those queries pack quite a punch- a number of things are happening. Let's unpack what they do.\n",
    "1. Convert the existing column's data type from string to a Spark-supported timestamp.\n",
    "2. use the new format specified in the format string \"MM/dd/yyyy\" or \"MM/dd/yyyy hh:mm:ss a\" where appropriate\n",
    "3. After converting to the new data type, `drop()` the old column and append the new one specified in the first argument to the `withColumn()` method.\n",
    "4. Assign the new modified DataFrame to `fire_ts_df`.\n",
    "\n",
    "The queries would result in 3 new columns in the above examples. \n",
    "\n",
    "Now that we have modified the dates, we can query using functions from `spark.sql.functions` like `month()`, `year()`, and `day()` to explore our data further. We could find out how many calls were logged in the last seven days, or we could see how many years’ worth of Fire Department calls are included in the data set with this query:\n",
    "\n",
    "    # In Python\n",
    "    (fire_ts_df\n",
    "      .select(year('IncidentDate'))\n",
    "      .distinct()\n",
    "      .orderBy(year('IncidentDate'))\n",
    "      .show())\n",
    "\n",
    "    // In Scala\n",
    "    fireTsDF\n",
    "      .select(year(4\"IncidentDate\"))\n",
    "      .distinct()\n",
    "      .orderBy(year(4\"IncidentDate\"))\n",
    "      .show()\n",
    "      \n",
    "One final common operation is grouping data by values in a column and aggregating the data in some way, like simply counting it. This pattern of grouping and counting is as common as projecting and filtering.\n",
    "\n",
    "### Aggregations\n",
    "\n",
    "What if we want to know what the most common types of fire calls were, or what zip codes accounted for the most calls? These kinds of questions are common in data analysis and exploration. A handful of transformations and actions on DataFrames, such as `groupBy()`, `orderBy()`, and `count()`, offer the ability to aggregate by column names and then aggregate counts across them. For larger DataFrame on which you plan to conduct freqent or repeated queries, you could benefit from caching.\n",
    "\n",
    "    # In Python\n",
    "    (fire_ts_df\n",
    "      .select(\"CallType\")\n",
    "      .where(col(\"CallType\").isNotNull())\n",
    "      .groupBy(\"CallType\")\n",
    "      .count()\n",
    "      .orderBy(\"count\", ascending=False)\n",
    "      .show(n=10, truncate=False))\n",
    "      \n",
    "    // In Scala\n",
    "    fireTsDF\n",
    "        .select(\"CallType\") \n",
    "        .where(col(\"CallType\").isNotNull) \n",
    "        .groupBy(\"CallType\")\n",
    "        .count()\n",
    "        .orderBy(desc(\"count\"))\n",
    "        .show(10, false)\n",
    "\n",
    "\n",
    "The DataFrame API also offers the `collect()` method, but for extremely large DataFrames this is resource-heavy (expensive) and dangerous, as it can cause out-of-memory (OOM) exceptions. Unlike `count()`, which returns a single number to the driver, `collect()` returns a collection of all the `Row` objects in the entire Data‐ Frame or Dataset. If you want to take a peek at some Row records you’re better off with `take(n)`, which will return only the first `n Row` objects of the DataFrame\n",
    "\n",
    "### Other Common DataFrame Operations.\n",
    "\n",
    "Along with all the others above, the DataFrame APi provides descriptive statistical methodds like `min()`, `max()`, `sum()` and `avg()`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
